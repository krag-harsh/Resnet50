{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_ds= torchvision.datasets.CIFAR10(root=\"data/\", train = True, transform=transforms.ToTensor(), download=True)\n",
    "test_ds= torchvision.datasets.CIFAR10(root=\"data/\", train = False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_ds) = 50000\n",
    "# len(train_loader) = 391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_loader= DataLoader(train_ds, batch_size, shuffle =True)\n",
    "test_leader=  DataLoader(test_ds, batch_size, shuffle = False)\n",
    "# len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:1\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "num_classes=10\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, inplane, planes, stride=1):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        outplane=4*planes\n",
    "        self.conv1 =   nn.Conv2d(inplane, planes,kernel_size=1,bias=False)  #same convolution\n",
    "        self.batchN1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 =   nn.Conv2d(planes, planes,kernel_size=3 ,stride=stride, padding=1 ,bias=False) #if stride is 1 then same convolution\n",
    "        self.batchN2 =  nn.BatchNorm2d(planes)\n",
    "        self.conv3 =   nn.Conv2d(planes, outplane,kernel_size=1,bias=False) #same convolution\n",
    "        self.batchN3 =  nn.BatchNorm2d(outplane)\n",
    "        \n",
    "        self.shortcuts=nn.Sequential()  #creating a shortcut which will help in residual part\n",
    "        if(outplane!=inplane or stride!=1):  #we make the shortcut so that the dimension match \n",
    "            self.shortcuts = nn.Sequential(nn.Conv2d(inplane, outplane, kernel_size=1,stride=stride,bias=False) ,nn.BatchNorm2d(outplane))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=F.relu(self.batchN1(self.conv1(x)))\n",
    "        out=F.relu(self.batchN2(self.conv2(out)))\n",
    "        out=self.batchN3(self.conv3(out))\n",
    "        out=F.relu(out+self.shortcuts(x))   #most important step of residual network\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resmodel(nn.Module):\n",
    "    # def __init__(self, block=Bottleneck, no_bl=[3,4,6,3], output_no=10):\n",
    "    def __init__(self, block, no_bl=[3,4,6,3], output_no=10):\n",
    "        super(Resmodel,self).__init__()\n",
    "        self.in_plane=64\n",
    "        self.conv1= nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1 ,bias=False)\n",
    "        self.batchN11= nn.BatchNorm2d(64)\n",
    "        self.layer1= self._make_layer(block, 64, 3, 1)\n",
    "        self.layer2= self._make_layer(block, 128, 4, 2)\n",
    "        self.layer3= self._make_layer(block, 256, 6, 2)\n",
    "        self.layer4= self._make_layer(block, 512, 3, 2)\n",
    "        self.linear = nn.Linear(2048,10)    #finally we will classify in 10 class,hence we have 10 dimension in final layer\n",
    "        \n",
    "    def _make_layer(self, block, plane, numofblock, stride):\n",
    "        layers=[]\n",
    "        strides= [stride] + [1]*(numofblock-1) #first block will have the given stride rest will have stride=1\n",
    "        #we have stride only for the first bottleneck(that too only one layer of bottleneck will use it)(hence the reduction of dimension due to stride will take place only once for one block of bottleneck)\n",
    "        for st in strides:\n",
    "            layers.append(Bottleneck(self.in_plane, plane, st))\n",
    "            self.in_plane = plane*4     #we know that the last layer in bottleneck is 4 times the size of input layer, hence we multiply it here, so that the first layer of next bottleneck will match the dimension of last layer of previous bottleneck.\n",
    "        return nn.Sequential(*layers)   #*layers will return values stored in the list\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #size of x=[batchsize,3,32,32]\n",
    "        out=self.conv1(x)           #after this operation size=[batchsize,64,32,32]\n",
    "        out=self.batchN11(out)\n",
    "        out=F.relu(out)\n",
    "        # out= F.relu(self.batchN11(self.conv1(x)))\n",
    "        out=self.layer1(out)        #after this operation the output size must be [batchsize,256,32,32]\n",
    "        out=self.layer2(out)        #after this operation the output size must be [batchsize,512,16,16]\n",
    "        out=self.layer3(out)        #after this operation the output size must be [batchsize,1024,8,8]\n",
    "        out=self.layer4(out)        #after this operation the output size must be [batchsize,2048,4,4]\n",
    "        out=F.avg_pool2d(out,4)     #after this operation the output size must be [batchsize,2048,1,1]\n",
    "        out= out.view(out.size(0),-1)    #after this operation the output size must be [batchsize,1,2048]\n",
    "        out=self.linear(out)         #after this operation the output size must be [batchsize,1,10]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Resmodel(Bottleneck)\n",
    "model=model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer= optim.SGD(model.parameters(), lr= 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/2], Step [16/391], Loss: 0.0922\n",
      "Epoch [1/2], Step [32/391], Loss: 0.1403\n",
      "Epoch [1/2], Step [48/391], Loss: 0.0884\n",
      "Epoch [1/2], Step [64/391], Loss: 0.1015\n",
      "Epoch [1/2], Step [80/391], Loss: 0.1626\n",
      "Epoch [1/2], Step [96/391], Loss: 0.1563\n",
      "Epoch [1/2], Step [112/391], Loss: 0.1998\n",
      "Epoch [1/2], Step [128/391], Loss: 0.1411\n",
      "Epoch [1/2], Step [144/391], Loss: 0.1463\n",
      "Epoch [1/2], Step [160/391], Loss: 0.2057\n",
      "Epoch [1/2], Step [176/391], Loss: 0.1315\n",
      "Epoch [1/2], Step [192/391], Loss: 0.1081\n",
      "Epoch [1/2], Step [208/391], Loss: 0.1253\n",
      "Epoch [1/2], Step [224/391], Loss: 0.1002\n",
      "Epoch [1/2], Step [240/391], Loss: 0.2070\n",
      "Epoch [1/2], Step [256/391], Loss: 0.1577\n",
      "Epoch [1/2], Step [272/391], Loss: 0.1727\n",
      "Epoch [1/2], Step [288/391], Loss: 0.3206\n",
      "Epoch [1/2], Step [304/391], Loss: 0.1674\n",
      "Epoch [1/2], Step [320/391], Loss: 0.1892\n",
      "Epoch [1/2], Step [336/391], Loss: 0.2041\n",
      "Epoch [1/2], Step [352/391], Loss: 0.0818\n",
      "Epoch [1/2], Step [368/391], Loss: 0.1765\n",
      "Epoch [1/2], Step [384/391], Loss: 0.2955\n",
      "Epoch [2/2], Step [16/391], Loss: 0.1167\n",
      "Epoch [2/2], Step [32/391], Loss: 0.0685\n",
      "Epoch [2/2], Step [48/391], Loss: 0.1259\n",
      "Epoch [2/2], Step [64/391], Loss: 0.1082\n",
      "Epoch [2/2], Step [80/391], Loss: 0.0836\n",
      "Epoch [2/2], Step [96/391], Loss: 0.1026\n",
      "Epoch [2/2], Step [112/391], Loss: 0.1245\n",
      "Epoch [2/2], Step [128/391], Loss: 0.0880\n",
      "Epoch [2/2], Step [144/391], Loss: 0.1111\n",
      "Epoch [2/2], Step [160/391], Loss: 0.1265\n",
      "Epoch [2/2], Step [176/391], Loss: 0.1138\n",
      "Epoch [2/2], Step [192/391], Loss: 0.0889\n",
      "Epoch [2/2], Step [208/391], Loss: 0.1381\n",
      "Epoch [2/2], Step [224/391], Loss: 0.2767\n",
      "Epoch [2/2], Step [240/391], Loss: 0.1668\n",
      "Epoch [2/2], Step [256/391], Loss: 0.2047\n",
      "Epoch [2/2], Step [272/391], Loss: 0.1154\n",
      "Epoch [2/2], Step [288/391], Loss: 0.0759\n",
      "Epoch [2/2], Step [304/391], Loss: 0.1137\n",
      "Epoch [2/2], Step [320/391], Loss: 0.1513\n",
      "Epoch [2/2], Step [336/391], Loss: 0.1677\n",
      "Epoch [2/2], Step [352/391], Loss: 0.1796\n",
      "Epoch [2/2], Step [368/391], Loss: 0.2559\n",
      "Epoch [2/2], Step [384/391], Loss: 0.1855\n",
      "Finished Training\n",
      "CPU times: user 1min 53s, sys: 42.6 s, total: 2min 36s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(2):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [128, 3, 32, 32]\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 16 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{2}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# print('max value of i=',max_val_i)\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network: 69.88 %\nAccuracy of plane: 71.2 %\nAccuracy of car: 85.5 %\nAccuracy of bird: 65.5 %\nAccuracy of cat: 69.3 %\nAccuracy of deer: 65.9 %\nAccuracy of dog: 27.6 %\nAccuracy of frog: 79.3 %\nAccuracy of horse: 69.4 %\nAccuracy of ship: 85.2 %\nAccuracy of truck: 79.9 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_leader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # for i in range(batch_size):\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_leader) = 79\n",
    "# len(images)\n",
    "# 79*128 almost equals 10k\n",
    "# print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# y = torch.randn(1, 3, 4,4)\n",
    "# out=F.avg_pool2d(y,4)\n",
    "# print(out.shape)\n",
    "# out= out.view(out.size(0),-1)\n",
    "# print(\"new shape=\",out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}