{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "\n",
    "# import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "random.seed(100)\n",
    "\n",
    "train_ds= torchvision.datasets.CIFAR10(root=\"data/\", train = True, transform=transforms.ToTensor(), download=True)\n",
    "test_ds= torchvision.datasets.CIFAR10(root=\"data/\", train = False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_loader= DataLoader(train_ds, batch_size, shuffle =True)\n",
    "test_leader=  DataLoader(test_ds, batch_size, shuffle = False)\n",
    "# len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:1\" if torch.cuda.is_available else \"cpu\")\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "num_classes=10\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, inplane, planes, stride=1):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        outplane=4*planes\n",
    "        self.conv1 =   nn.Conv2d(inplane, planes,kernel_size=1,bias=False)  #same convolution\n",
    "        self.batchN1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 =   nn.Conv2d(planes, planes,kernel_size=3 ,stride=stride, padding=1 ,bias=False) #if stride is 1 then same convolution\n",
    "        self.batchN2 =  nn.BatchNorm2d(planes)\n",
    "        self.conv3 =   nn.Conv2d(planes, outplane,kernel_size=1,bias=False) #same convolution\n",
    "        self.batchN3 =  nn.BatchNorm2d(outplane)\n",
    "        \n",
    "        self.shortcuts=nn.Sequential()  #creating a shortcut which will help in residual part\n",
    "        if(outplane!=inplane or stride!=1):  #we make the shortcut so that the dimension match \n",
    "            self.shortcuts = nn.Sequential(nn.Conv2d(inplane, outplane, kernel_size=1,stride=stride,bias=False) ,nn.BatchNorm2d(outplane))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=F.relu(self.batchN1(self.conv1(x)))\n",
    "        out=F.relu(self.batchN2(self.conv2(out)))\n",
    "        out=self.batchN3(self.conv3(out))\n",
    "        out=F.relu(out+self.shortcuts(x))   #most important step of residual network\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resmodel(nn.Module):\n",
    "    # def __init__(self, block=Bottleneck, no_bl=[3,4,6,3], output_no=10):\n",
    "    def __init__(self, block, no_bl=[3,4,6,3], output_no=10):\n",
    "        super(Resmodel,self).__init__()\n",
    "        self.in_plane=64\n",
    "        self.conv1= nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1 ,bias=False)\n",
    "        self.batchN11= nn.BatchNorm2d(64)\n",
    "        self.layer1= self._make_layer(block, 64, 3, 1)\n",
    "        self.layer2= self._make_layer(block, 128, 4, 2)\n",
    "        self.layer3= self._make_layer(block, 256, 6, 2)\n",
    "        self.layer4= self._make_layer(block, 512, 3, 2)\n",
    "        self.linear = nn.Linear(2048,10)    #finally we will classify in 10 class,hence we have 10 dimension\n",
    "        \n",
    "    def _make_layer(self, block, plane, numofblock, stride):\n",
    "        layers=[]\n",
    "        strides= [stride] + [1]*(numofblock-1) #first block will have the given stride rest will have stride=1\n",
    "        #we have stride only for the first bottleneck(that too only one layer of bottleneck will use it)(hence the reduction of dimension due to stride will take place only once for one block of bottleneck)\n",
    "        for st in strides:\n",
    "            layers.append(Bottleneck(self.in_plane, plane, st))\n",
    "            self.in_plane = plane*4     #we know that the last layer in bottleneck is 4 times the size of input layer, hence we multiply it here, so that the first layer of next bottleneck will match the dimension of last layer of previous bottleneck.\n",
    "        return nn.Sequential(*layers)   #*layers will return values stored in the list\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #size of x=[batchsize,3,32,32]\n",
    "        out=self.conv1(x)           #after this operation size=[batchsize,64,32,32]\n",
    "        out=self.batchN11(out)\n",
    "        out=F.relu(out)\n",
    "        # out= F.relu(self.batchN11(self.conv1(x)))\n",
    "        out=self.layer1(out)        #after this operation the output size must be [batchsize,256,32,32]\n",
    "        out=self.layer2(out)        #after this operation the output size must be [batchsize,512,16,16]\n",
    "        out=self.layer3(out)        #after this operation the output size must be [batchsize,1024,8,8]\n",
    "        out=self.layer4(out)        #after this operation the output size must be [batchsize,2048,4,4]\n",
    "        out=F.avg_pool2d(out,4)     #after this operation the output size must be [batchsize,2048,1,1]\n",
    "        out= out.view(out.size(0),-1)    #after this operation the output size must be [batchsize,1,2048]\n",
    "        out=self.linear(out)         #after this operation the output size must be [batchsize,1,10]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try focal losss\n",
    "\n",
    "class focalloss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(focalloss,self).__init__()\n",
    "\n",
    "    def forward(self, inp,targ, alpha=1, gamma=2):\n",
    "        \n",
    "        # inp=F.sigmoid(inp)\n",
    "        # lloss=F.cross_entropy(inp,targ,reduction='sum')\n",
    "        lloss=F.cross_entropy(inp,targ)\n",
    "        # inp=inp.view(-1)\n",
    "        \n",
    "        # targ=targ.view(-1)\n",
    "        # Biloss=F.binary_cross_entropy(inp,targ, reduction='mean')\n",
    "        ll_exp=torch.exp(-lloss)\n",
    "        focalLoss=alpha*(1-ll_exp)**gamma * lloss\n",
    "        return focalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Resmodel(Bottleneck)\n",
    "model=model.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion=focalloss()\n",
    "optimizer= optim.SGD(model.parameters(), lr= 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/2], Step [16/391], Loss: 2.1032\n",
      "Epoch [1/2], Step [32/391], Loss: 2.3864\n",
      "Epoch [1/2], Step [48/391], Loss: 1.7667\n",
      "Epoch [1/2], Step [64/391], Loss: 1.9700\n",
      "Epoch [1/2], Step [80/391], Loss: 1.5461\n",
      "Epoch [1/2], Step [96/391], Loss: 1.5713\n",
      "Epoch [1/2], Step [112/391], Loss: 1.6566\n",
      "Epoch [1/2], Step [128/391], Loss: 1.4139\n",
      "Epoch [1/2], Step [144/391], Loss: 1.7263\n",
      "Epoch [1/2], Step [160/391], Loss: 1.2902\n",
      "Epoch [1/2], Step [176/391], Loss: 1.3191\n",
      "Epoch [1/2], Step [192/391], Loss: 1.3928\n",
      "Epoch [1/2], Step [208/391], Loss: 1.0788\n",
      "Epoch [1/2], Step [224/391], Loss: 1.2690\n",
      "Epoch [1/2], Step [240/391], Loss: 0.9117\n",
      "Epoch [1/2], Step [256/391], Loss: 1.0811\n",
      "Epoch [1/2], Step [272/391], Loss: 1.0344\n",
      "Epoch [1/2], Step [288/391], Loss: 0.8762\n",
      "Epoch [1/2], Step [304/391], Loss: 1.0268\n",
      "Epoch [1/2], Step [320/391], Loss: 0.9609\n",
      "Epoch [1/2], Step [336/391], Loss: 1.1074\n",
      "Epoch [1/2], Step [352/391], Loss: 1.0937\n",
      "Epoch [1/2], Step [368/391], Loss: 0.7280\n",
      "Epoch [1/2], Step [384/391], Loss: 0.9043\n",
      "Epoch [2/2], Step [16/391], Loss: 0.7741\n",
      "Epoch [2/2], Step [32/391], Loss: 1.0109\n",
      "Epoch [2/2], Step [48/391], Loss: 0.7396\n",
      "Epoch [2/2], Step [64/391], Loss: 0.7407\n",
      "Epoch [2/2], Step [80/391], Loss: 0.7489\n",
      "Epoch [2/2], Step [96/391], Loss: 0.9077\n",
      "Epoch [2/2], Step [112/391], Loss: 0.9563\n",
      "Epoch [2/2], Step [128/391], Loss: 1.0240\n",
      "Epoch [2/2], Step [144/391], Loss: 0.8475\n",
      "Epoch [2/2], Step [160/391], Loss: 0.6548\n",
      "Epoch [2/2], Step [176/391], Loss: 0.7409\n",
      "Epoch [2/2], Step [192/391], Loss: 0.7499\n",
      "Epoch [2/2], Step [208/391], Loss: 0.6678\n",
      "Epoch [2/2], Step [224/391], Loss: 0.6931\n",
      "Epoch [2/2], Step [240/391], Loss: 0.7049\n",
      "Epoch [2/2], Step [256/391], Loss: 0.8236\n",
      "Epoch [2/2], Step [272/391], Loss: 0.6743\n",
      "Epoch [2/2], Step [288/391], Loss: 0.7394\n",
      "Epoch [2/2], Step [304/391], Loss: 0.4727\n",
      "Epoch [2/2], Step [320/391], Loss: 0.6616\n",
      "Epoch [2/2], Step [336/391], Loss: 0.6851\n",
      "Epoch [2/2], Step [352/391], Loss: 0.6039\n",
      "Epoch [2/2], Step [368/391], Loss: 0.5717\n",
      "Epoch [2/2], Step [384/391], Loss: 0.7887\n",
      "Finished Training\n",
      "CPU times: user 2min, sys: 38.5 s, total: 2min 38s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_total_steps = len(train_loader)\n",
    "NOepo=2\n",
    "for epoch in range(NOepo):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [128, 3, 32, 32]\n",
    "        images = images.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        #for focal loss:\n",
    "        # labels= F.one_hot(labels, num_classes=10)       \n",
    "        # labels= labels.type(torch.FloatTensor)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        # break\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 16 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{NOepo}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# print('max value of i=',max_val_i)\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network: 53.09 %\nAccuracy of plane: 66.0 %\nAccuracy of car: 88.0 %\nAccuracy of bird: 26.1 %\nAccuracy of cat: 64.9 %\nAccuracy of deer: 35.5 %\nAccuracy of dog: 25.3 %\nAccuracy of frog: 64.5 %\nAccuracy of horse: 66.1 %\nAccuracy of ship: 63.4 %\nAccuracy of truck: 31.1 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_leader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # for i in range(batch_size):\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}